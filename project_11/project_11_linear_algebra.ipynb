{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Title: Analysis for The Sure Tomorrow Insurance Company "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sure Tomorrow insurance company wants to solve several tasks with the help of Machine Learning and you are asked to evaluate that possibility.\n",
    "\n",
    "- Task 1: Find customers who are similar to a given customer. This will help the company's agents with marketing.\n",
    "- Task 2: Predict whether a new customer is likely to receive an insurance benefit. Can a prediction model do better than a dummy model?\n",
    "- Task 3: Predict the number of insurance benefits a new customer is likely to receive using a linear regression model.\n",
    "- Task 4: Protect clients' personal data without breaking the model from the previous task. It's necessary to develop a data transformation algorithm that would make it hard to recover personal information if the data fell into the wrong hands. This is called data masking, or data obfuscation. But the data should be protected in such a way that the quality of machine learning models doesn't suffer. You don't need to pick the best model, just prove that the algorithm works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Exploration\n",
    "\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.neighbors\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors \n",
    "from sklearn.preprocessing import MaxAbsScaler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and conduct a basic check that it's free from obvious issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/datasets/insurance_us.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rename the colums to make the code look more consistent with its style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Gender': 'gender', 'Age': 'age', 'Salary': 'income', 'Family members': 'family_members', 'Insurance benefits': 'insurance_benefits'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may want to fix the age type (from float to int) though this is not critical\n",
    "\n",
    "# write your conversion here if you choose:\n",
    "df['age'] = df['age'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see that the conversion was successful\n",
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now have a look at the data's descriptive statistics. \n",
    "# Does everything look okay?\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: There are no missing values. However, it is difficult to determine if there are true duplicates. Therefore, the data will be left in it's present condition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check whether there are certain groups of customers by looking at the pair plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df, kind='hist')\n",
    "g.fig.set_size_inches(12, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it is a bit difficult to spot obvious groups (clusters) as it is difficult to combine several variables simultaneously (to analyze multivariate distributions). That's where LA and ML can be quite handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Similar Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the language of ML, it is necessary to develop a procedure that returns k nearest neighbors (objects) for a given object based on the distance between the objects.\n",
    "\n",
    "You may want to review the following lessons (chapter -> lesson)\n",
    "- Distance Between Vectors -> Euclidean Distance\n",
    "- Distance Between Vectors -> Manhattan Distance\n",
    "\n",
    "To solve the task, we can try different distance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that returns k nearest neighbors for an $n^{th}$ object based on a specified distance metric. The number of received insurance benefits should not be taken into account for this task. \n",
    "\n",
    "You can use a ready implementation of the kNN algorithm from scikit-learn (check [the link](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors)) or use your own.\n",
    "\n",
    "Test it for four combination of two cases\n",
    "- Scaling\n",
    "  - the data is not scaled\n",
    "  - the data is scaled with the [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html) scaler\n",
    "- Distance Metrics\n",
    "  - Euclidean\n",
    "  - Manhattan\n",
    "\n",
    "Answer these questions:\n",
    "- Does the data being not scaled affect the kNN algorithm? If so, how does that appear?\n",
    "- How similar are the results using the Manhattan distance metric (regardless of the scaling)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['gender', 'age', 'income', 'family_members']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn(df, n, k, metric):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns k nearest neighbors\n",
    "\n",
    "    :param df: pandas DataFrame used to find similar objects within\n",
    "    :param n: object no for which the nearest neighbours are looked for\n",
    "    :param k: the number of the nearest neighbours to return\n",
    "    :param metric: name of distance metric\n",
    "    \"\"\"\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric=metric) \n",
    "\n",
    "    nbrs.fit(df[feature_names]) \n",
    "    \n",
    "    nbrs_distances, nbrs_indices = nbrs.kneighbors([df.iloc[n][feature_names]], k, return_distance=True)\n",
    "    \n",
    "    df_res = pd.concat([\n",
    "        df.iloc[nbrs_indices[0]], \n",
    "        pd.DataFrame(nbrs_distances.T, index=nbrs_indices[0], columns=['distance'])\n",
    "        ], axis=1)\n",
    "    \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['gender', 'age', 'income', 'family_members']\n",
    "\n",
    "transformer_mas = sklearn.preprocessing.MaxAbsScaler().fit(df[feature_names].to_numpy())\n",
    "\n",
    "df_scaled = df.copy()\n",
    "df_scaled.loc[:, feature_names] = transformer_mas.transform(df[feature_names].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unscaled Euclidean distance \n",
    "nbrs_unscaled_euclidean = get_knn(df, n=0, k=5, metric='euclidean')\n",
    "print(\"Unscaled data with Euclidean distance:\")\n",
    "print(nbrs_unscaled_euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unscaled Manhattan distance \n",
    "nbrs_unscaled_euclidean = get_knn(df, n=0, k=5, metric='euclidean')\n",
    "print(\"Unscaled data with Euclidean distance:\")\n",
    "print(nbrs_unscaled_euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling Euclidean distance \n",
    "nbrs_scaled_manhattan = get_knn(df_scaled, n=0, k=5, metric='euclidean')\n",
    "print(\"\\nScaled data with Manhattan distance:\")\n",
    "print(nbrs_scaled_manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling Manhattan distance \n",
    "nbrs_scaled_manhattan = get_knn(df_scaled, n=0, k=5, metric='manhattan')\n",
    "print(\"\\nScaled data with Manhattan distance:\")\n",
    "print(nbrs_scaled_manhattan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get similar records for a given one for every combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers to the questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does the data being not scaled affect the kNN algorithm? If so, how does that appear?** \n",
    "\n",
    "Yes, the unscaled data will impact the kNN algorithm distances. The features with greater numerical ranges  can effect the distance calculation. This results in different nearest neighbors being selected. The scaled data ensures that the features remain in comparable ranges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How similar are the results using the Manhattan distance metric (regardless of the scaling)?** \n",
    "\n",
    "The results are very consistent when using the Manhattan distance metric because the absolute value is used. Absolute value ensures consistency because each feature is weighted equally. This also prevents outliers from having as big an impact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Is Customer Likely to Receive Insurance Benefit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of machine learning we can look at this like a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `insurance_benefits` being more than zero as the target, evaluate whether the kNN classification approach can do better than a dummy model.\n",
    "\n",
    "Instructions:\n",
    "- Build a KNN-based classifier and measure its quality with the F1 metric for k=1..10 for both the original data and the scaled one. That'd be interesting to see how k may influece the evaluation metric, and whether scaling the data makes any difference. You can use a ready implemention of the kNN classification algorithm from scikit-learn (check [the link](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)) or use your own.\n",
    "- Build the dummy model which is just random for this case. It should return \"1\" with some probability. Let's test the model with four probability values: 0, the probability of paying any insurance benefit, 0.5, 1.\n",
    "\n",
    "The probability of paying any insurance benefit can be defined as\n",
    "\n",
    "$$\n",
    "P\\{\\text{insurance benefit received}\\}=\\frac{\\text{number of clients received any insurance benefit}}{\\text{total number of clients}}.\n",
    "$$\n",
    "\n",
    "Split the whole data in the 70:30 proportion for the training/testing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the target\n",
    "\n",
    "df['insurance_benefits_received'] = (df['insurance_benefits']>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the class imbalance with value_counts()\n",
    "df['insurance_benefits_received'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(y_true, y_pred):\n",
    "    \n",
    "    f1_score = sklearn.metrics.f1_score(y_true, y_pred)\n",
    "    print(f'F1: {f1_score:.2f}')\n",
    "    \n",
    "# if you have an issue with the following line, restart the kernel and run the notebook again\n",
    "    cm = sklearn.metrics.confusion_matrix(y_true, y_pred, normalize='all')\n",
    "    print('Confusion Matrix')\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating output of a random model\n",
    "\n",
    "def rnd_model_predict(P, size, seed=42):\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    return rng.binomial(n=1, p=P, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for P in [0, df['insurance_benefits_received'].sum() / len(df), 0.5, 1]:\n",
    "\n",
    "    print(f'The probability: {P:.2f}')\n",
    "    y_pred_rnd = rnd_model_predict(P, size=len(df))\n",
    "        \n",
    "    eval_classifier(df['insurance_benefits_received'], y_pred_rnd)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "#split data into training and validation sets \n",
    "drop_columns = ['insurance_benefits_received','insurance_benefits']\n",
    "X = df.drop(drop_columns, axis=1) \n",
    "y = df['insurance_benefits_received'] \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "\n",
    "#print shapes of training and validation sets to verify \n",
    "print(\"Training set:\") \n",
    "print(X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\")\n",
    "print(X_val.shape, y_val.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize MaxAbsScaler and fit transform on training data \n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "X_val_scaled = scaler.transform(X_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "#Build kNN classifier \n",
    "\n",
    "def evaluate_knn(X_train, y_train, X_val, y_val): \n",
    "    for k in range(1,11): \n",
    "        knn = KNeighborsClassifier(n_neighbors=k) \n",
    "        knn.fit(X_train, y_train) \n",
    "        y_pred = knn.predict(X_val) \n",
    "        print(f' k={k}') \n",
    "        eval_classifier(y_val, y_pred) \n",
    "        print() \n",
    "\n",
    "#Examine original data \n",
    "print(\"Original data:\") \n",
    "evaluate_knn(X_train, y_train, X_val, y_val) \n",
    "\n",
    "print()\n",
    "\n",
    "#Examine scaled data \n",
    "print(\"Scaled data:\") \n",
    "evaluate_knn(X_train_scaled, y_train, X_val_scaled, y_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Regression (with Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `insurance_benefits` as the target, evaluate what RMSE would be for a Linear Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your own implementation of LR. For that, recall how the linear regression task's solution is formulated in terms of LA. Check RMSE for both the original data and the scaled one. Can you see any difference in RMSE between these two cases?\n",
    "\n",
    "Let's denote\n",
    "- $X$ — feature matrix, each row is a case, each column is a feature, the first column consists of unities\n",
    "- $y$ — target (a vector)\n",
    "- $\\hat{y}$ — estimated tagret (a vector)\n",
    "- $w$ — weight vector\n",
    "\n",
    "The task of linear regression in the language of matrices can be formulated as\n",
    "\n",
    "$$\n",
    "y = Xw\n",
    "$$\n",
    "\n",
    "The training objective then is to find such $w$ that it would minimize the L2-distance (MSE) between $Xw$ and $y$:\n",
    "\n",
    "$$\n",
    "\\min_w d_2(Xw, y) \\quad \\text{or} \\quad \\min_w \\text{MSE}(Xw, y)\n",
    "$$\n",
    "\n",
    "It appears that there is analytical solution for the above:\n",
    "\n",
    "$$\n",
    "w = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "The formula above can be used to find the weights $w$ and the latter can be used to calculate predicted values\n",
    "\n",
    "$$\n",
    "\\hat{y} = X_{val}w\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the whole data in the 70:30 proportion for the training/validation parts. Use the RMSE metric for the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.weights = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # adding the unities\n",
    "        X2 = np.append(np.ones([len(X), 1]), X, axis=1)\n",
    "        self.weights = np.linalg.inv(X2.T @ X2) @ X2.T @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # adding the unities\n",
    "        X2 = np.append(np.ones([len(X),1]),X, axis=1)\n",
    "        y_pred = X2 @ self.weights\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_regressor(y_true, y_pred):\n",
    "    \n",
    "    rmse = math.sqrt(sklearn.metrics.mean_squared_error(y_true, y_pred))\n",
    "    print(f'RMSE: {rmse:.2f}')\n",
    "    \n",
    "    r2_score = math.sqrt(sklearn.metrics.r2_score(y_true, y_pred))\n",
    "    print(f'R2: {r2_score:.2f}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['age', 'gender', 'income', 'family_members']].to_numpy()\n",
    "y = df['insurance_benefits'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "\n",
    "lr = MyLinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.weights)\n",
    "\n",
    "y_test_pred = lr.predict(X_test)\n",
    "eval_regressor(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled data \n",
    "X = df_scaled[['age','gender','income','family_members']].to_numpy()\n",
    "y = df_scaled['insurance_benefits'].to_numpy() \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=12345)\n",
    "\n",
    "lr=MyLinearRegression() \n",
    "\n",
    "lr.fit(X_train, y_train) \n",
    "print(f\"Scaled Data:\")\n",
    "print(f\"Weights:\")\n",
    "print(lr.weights)\n",
    "\n",
    "print() \n",
    "\n",
    "print(f\"Scaled Data:\") \n",
    "\n",
    "y_test_pred = lr.predict(X_test) \n",
    "\n",
    "eval_regressor(y_test, y_test_pred) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: \n",
    "The weights differ between the original and scaled data. This is due to the magnitude of the features and coefficients of the regression model. The RMSE did not change indicating that the model performs equally well with both scaled and unscaled data. In addition the R2 value did not change ethier suggesting that the proportion of varaince is consistent irregardless of scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. Obfuscating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It best to obfuscate data by multiplying the numerical features (remember, they can be seen as the matrix $X$) by an invertible matrix $P$. \n",
    "\n",
    "$$\n",
    "X' = X \\times P\n",
    "$$\n",
    "\n",
    "Try to do that and check how the features' values will look like after the transformation. By the way, the intertible property is important here so make sure that $P$ is indeed invertible.\n",
    "\n",
    "You may want to review the 'Matrices and Matrix Operations -> Matrix Multiplication' lesson to recall the rule of matrix multiplication and its implementation with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_info_column_list = ['gender', 'age', 'income', 'family_members']\n",
    "df_pn = df[personal_info_column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_pn.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a random matrix $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "P = rng.random(size=(X.shape[1], X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the matrix $P$ is invertible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while np.linalg.det(P) == 0:\n",
    "    P = rng.random(size=(X.shape[1],X.shape[1])) \n",
    "X_transformed = X @ P \n",
    "\n",
    "df_transformed = pd.DataFrame(X_transformed, columns=personal_info_column_list) \n",
    "\n",
    "print(\"Original Data:\") \n",
    "print(df_pn.head()) \n",
    "print(\"Transformed Data:\") \n",
    "print(df_transformed.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you guess the customers' ages or income after the transformation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is no possible to guess the customers' age or income after the transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you recover the original data from $X'$ if you know $P$? Try to check that with calculations by moving $P$ from the right side of the formula above to the left one. The rules of matrix multiplcation are really helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_inv = np.linalg.inv(P) \n",
    "X_recovered = X_transformed @ P_inv \n",
    "\n",
    "df_transformed = pd.DataFrame(X_transformed, columns=personal_info_column_list)\n",
    "df_recovered =  pd.DataFrame(X_recovered,columns=personal_info_column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all three cases for a few customers\n",
    "- The original data\n",
    "- The transformed one\n",
    "- The reversed (recovered) one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intial data:\")\n",
    "print(df_pn.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can probably see that some values are not exactly the same as they are in the original data. What might be the reason for that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformed data:\")\n",
    "print(df_transformed.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recovered data:\")\n",
    "print(df_recovered.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing initial data and recovered data are close\n",
    "print(\"Is the initial data and recovered data close?\") \n",
    "print(np.allclose(X,X_recovered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof That Data Obfuscation Can Work with LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression task has been solved with linear regression in this project. Your next task is to prove _analytically_ that the given obfuscation method won't affect linear regression in terms of predicted values i.e. their values will remain the same. Can you believe that? Well, you don't have to, you should prove it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the data is obfuscated and there is $X \\times P$ instead of just $X$ now. Consequently, there are other weights $w_P$ as\n",
    "$$\n",
    "w = (X^T X)^{-1} X^T y \\quad \\Rightarrow \\quad w_P = [(XP)^T XP]^{-1} (XP)^T y\n",
    "$$\n",
    "\n",
    "How would $w$ and $w_P$ be linked if you simplify the formula for $w_P$ above? \n",
    "\n",
    "What would be predicted values with $w_P$? \n",
    "\n",
    "What does that mean for the quality of linear regression if you measure it with RMSE?\n",
    "\n",
    "Check Appendix B Properties of Matrices in the end of the notebook. There are useful formulas in there!\n",
    "\n",
    "No code is necessary in this section, only analytical explanation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How would w and wP be linked if you simplify the formula for wP? \n",
    "\n",
    "If we simplify the formula the weight for the obfuscated data is: \n",
    "wp =[(XP)^T XP]^-1 (XP)^T y\n",
    "\n",
    "Then further simplify to: \n",
    "wp = [(P^T X^T)(XP)]^-1(P^T X^T)y\n",
    "\n",
    "Then: \n",
    "wp = [P^T( X^T X)P]^-1 P^T X^Ty\n",
    "\n",
    "In examining the matrix inversions and properties: [P^T( X^T X)P]^-1 can be simplified using the property:\n",
    "(ABC)^-1 = C^-1B^-1A^-1(P^T(X^TX)P)^-1 = P^-1(X^TX)^-1(P^T)^-1\n",
    "\n",
    "Then further simplified to: \n",
    "wp = [P^-1(X^T X)^-1 (P^T)^-1]P^T X^Ty\n",
    "\n",
    "wp = P^-1(X^T X)^-1(P^T)^-1 P^T X^T y \n",
    "\n",
    "(P^T)^-1 P^T = I which simplifies to wp = P^-1 (X^T X)^-1 X^T y \n",
    "\n",
    "The link between w and wp since w = (w=(X^T X)^-1 X^T y). w can be substituted in the equation for wp as follows:\n",
    "wp = P^-1 w \n",
    "\n",
    "Basically, the weights for the obfuscated data (wp) are the original weights (w) multiplied by the inverse of the obfuscation matrix (P^-1). \n",
    "\n",
    "The weights calculated for the obfuscated data(wp) are just the original weights(w) transformed by P's inverse. \n",
    "\n",
    "\n",
    "What would be predicted values with wP? \n",
    "\n",
    "The original predicted values:\n",
    "ŷ = Xw \n",
    "\n",
    "The predicted values with obfuscated data: \n",
    "ŷ' = (XP)wp \n",
    "\n",
    "Substitute wp: \n",
    "\n",
    "ŷ' = (XP)(P^-1 w)\n",
    "\n",
    "ŷ' = X(PP^-1) w)\n",
    "\n",
    "ŷ' = XIw \n",
    "\n",
    "ŷ' = Xw \n",
    "\n",
    "The above shows that the predicted values ŷ using the original data X and the weights w are the same predicted values ŷ using the obfuscated data X' and the weights wp. \n",
    "\n",
    "What does that mean for the quality of linear regression if you measure it with RMSE? \n",
    "RMSE is the same for both cases since predicted values are the same. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical proof**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove that ŷ = Xw and ŷ = X'wp give the same predicted values ŷ = ŷ: \n",
    "\n",
    "A. Linear Regression Weight Calc:\n",
    "   for the origianl data w = (X^T X)^-1 X^Ty\n",
    "   for the obfuscated data wp = (X'^T X')^-1 X'^T y\n",
    "\n",
    "   Sub in X' = X x P\n",
    "\n",
    "   wp = [(XP)^T (XP)^-1] (XP)^T y\n",
    "   \n",
    "B. Simplify the expression for wp: \n",
    "\n",
    "wp = [P^T X^T XP]^-1 P^T X^T y \n",
    "\n",
    "Using the property matrix inversion: \n",
    "(ABC)^-1 = C^-1 B^-1 A^-1(P^T(X^TX)P)^-1 = P^-1(X^TX)^-1(P^T)^-1\n",
    "\n",
    "Then substitute back: \n",
    "wp = P^-1 (X^T X)^-1 (P^T)^-1 P^T X^T y \n",
    "\n",
    "C. Further simplify: \n",
    "Using the identity matrix: I = (P^T)^-1 P^T \n",
    "wp can then be simplified as wp = P^-1 (X^T X)^-1 X^T y \n",
    "w is just (X^T X)^-1 X^T y \n",
    "Therefore, it can be written as \n",
    "wp = P^-1 w \n",
    "\n",
    "D. Predicted values \n",
    "The original predicted value is ŷ = Xw \n",
    "The obfuscated data predicted values ŷ = (XP)wp \n",
    "\n",
    "Sub in wp = P^-1 w  \n",
    "\n",
    "ŷ = (XP)(P^-1 w)\n",
    "\n",
    "Using the associative property of matrix multiplication:\n",
    "ŷ = X(PP^-1)w\n",
    "\n",
    "Since PP^-1 = I \n",
    "ŷ = XIw \n",
    "\n",
    "Consequently, \n",
    "ŷ = Xw \n",
    "\n",
    "The predicted values are the same irregardless, of weather the obfuscated data or the original data is used. This would suggest that the RMSE values will not be impacted. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Linear Regression With Data Obfuscation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prove Linear Regression can work computationally with the chosen obfuscation transformation.\n",
    "\n",
    "Build a procedure or a class that runs Linear Regression optionally with the obfuscation. You can use either a ready implementation of Linear Regression from sciki-learn or your own.\n",
    "\n",
    "Run Linear Regression for the original data and the obfuscated one, compare the predicted values and the RMSE, $R^2$ metric values. Is there any difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Procedure**\n",
    "\n",
    "- Create a square matrix $P$ of random numbers.\n",
    "- Check that it is invertible. If not, repeat the first point until we get an invertible matrix.\n",
    "- <! your comment here !>\n",
    "- Use $XP$ as the new feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_invertible_matrix(size, seed=42):\n",
    "    rng = np.random.default_rng(seed) \n",
    "\n",
    "    while True:\n",
    "        P = rng.random(size=(size,size)) \n",
    "        if np.linalg.det(P) !=0:\n",
    "            return P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show if invertible \n",
    "P = generate_invertible_matrix(size=4, seed=42) \n",
    "print(\"Random Invertible Matrix p:\\n\",P) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Distribution of elements in P \n",
    "plt.hist(P.ravel(), bins=30, edgecolor='k') \n",
    "plt.title('Distribution of Elements in P') \n",
    "plt.xlabel('Value') \n",
    "plt.ylabel('Frequency') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR on initial data \n",
    "X = df[['age','gender','income','family_members']].to_numpy()\n",
    "y = df ['insurance_benefits'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "lr =MyLinearRegression() \n",
    "lr.fit(X_train, y_train) \n",
    "print(\"Initial data:\") \n",
    "y_test_pred = lr.predict(X_test) \n",
    "\n",
    "eval_regressor(y_test,y_test_pred) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obfuscate data \n",
    "personal_info_column_list = ['age','gender','income','family_members']\n",
    "df_pn = df[personal_info_column_list]\n",
    "X= df_pn.to_numpy() \n",
    "X_transformed = X @ P \n",
    "df_transformed = pd.DataFrame(X_transformed, columns = personal_info_column_list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR on Obfuscate data \n",
    "X_obfuscated = df_transformed[['age','gender','income','family_members']].to_numpy()\n",
    "y_obfuscated = df['insurance_benefits'].to_numpy()\n",
    "\n",
    "X_train_obfuscated, X_test_obfuscated, y_train_obfuscated, y_test_obfuscated = train_test_split(X_obfuscated, y_obfuscated, test_size=0.3, random_state=12345)\n",
    "lr =MyLinearRegression() \n",
    "lr.fit(X_train_obfuscated, y_train_obfuscated)  \n",
    "print(\"Obfuscated data:\") \n",
    "y_test_pred = lr.predict(X_test_obfuscated) \n",
    "\n",
    "eval_regressor(y_test_obfuscated, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obfuscation process does not impact the performance of the Linear Regression model as the data from the initial and obfuscated data remain the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of linear regression under obfuscation was examined. When the obfuscation method was applied where the features matrix is multiplied by a random invertible matrix that the quality of the predictions was not reduced. It was discovered that the obfuscation method does not impact the preditions. This idea is supported by the fact that the predicted values from the linear regression model for both the intial and obfuscated data were very similar. Therefore, it can be concluded that the obfuscation process does not interfere with the predictive abilities of the model. Another finding was that there was no change in performance metrics. This would suggest that the model's able to take into account variability even when obfuscation is used. In addition, since the matrix is invertible the inital data can be recovered if need be. The matrix adds an additional security measure, but does not compromise the quality of the model. The results of this project demonstrate that the predicted values and evaluate metrics such as RMSE and R^2 are operational after obfuscation. This indicates that data protection techniques can effectively operate without hindering the machine learning model. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 'x' to check. Then press Shift+Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices \n",
    "\n",
    "## Appendix A: Writing Formulas in Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write formulas in your Jupyter Notebook in a markup language provided by a high-quality publishing system called $\\LaTeX$ (pronounced \"Lah-tech\"), and they will look like formulas in textbooks.\n",
    "\n",
    "To put a formula in a text, put the dollar sign (\\\\$) before and after the formula's text e.g. $\\frac{1}{2} \\times \\frac{3}{2} = \\frac{3}{4}$ or $y = x^2, x \\ge 1$.\n",
    "\n",
    "If a formula should be in its own paragraph, put the double dollar sign (\\\\$\\\\$) before and after the formula text e.g.\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i.\n",
    "$$\n",
    "\n",
    "The markup language of [LaTeX](https://en.wikipedia.org/wiki/LaTeX) is very popular among people who use formulas in their articles, books and texts. It can be complex but its basics are easy. Check this two page [cheatsheet](http://tug.ctan.org/info/undergradmath/undergradmath.pdf) for learning how to compose the most common formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B: Properties of Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices have many properties in Linear Algebra. A few of them are listed here which can help with the analytical proof in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>Distributivity</td><td>$A(B+C)=AB+AC$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Non-commutativity</td><td>$AB \\neq BA$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Associative property of multiplication</td><td>$(AB)C = A(BC)$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Multiplicative identity property</td><td>$IA = AI = A$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td></td><td>$A^{-1}A = AA^{-1} = I$\n",
    "</td>\n",
    "</tr>    \n",
    "<tr>\n",
    "<td></td><td>$(AB)^{-1} = B^{-1}A^{-1}$</td>\n",
    "</tr>    \n",
    "<tr>\n",
    "<td>Reversivity of the transpose of a product of matrices,</td><td>$(AB)^T = B^TA^T$</td>\n",
    "</tr>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
